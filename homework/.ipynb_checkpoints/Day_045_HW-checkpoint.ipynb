{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "你可能聽過 XGBoost/Light-GBM，這些都是資料科學競賽中最常用的機器學習模型，但其實這些演算法背後原理都是基於 Gradient-boosting 進而優化，強烈建議您對本日的課程與補充教材多花點時間閱讀與理解。核心概念就是透過計算梯度，來讓下一棵生成的樹能夠根據梯度方向，試圖讓 Loss 變得更小！\n",
    "\n",
    "\n",
    "\n",
    "本日作業請完整閱讀以下任一文獻即可：\n",
    "\n",
    "\n",
    "- [Kaggle 大師帶你了解梯度提升機原理 - 英文](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "- [完整的 Ensemble 概念 by 李宏毅教授](https://www.youtube.com/watch?v=tH9FH1DH5n0)\n",
    "- [深入了解 Gradient-boosting - 英文](https://explained.ai/gradient-boosting/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資料\n",
    "\n",
    "#### 梯度提升機原理 - 中文\n",
    "連結：https://ifun01.com/84A3FW7.html  \n",
    "文章中的殘差就是前面提到的 Loss，從範例中了解殘差是如何被修正的\n",
    "\n",
    "#### XGboost 作者講解原理 - 英文\n",
    "連結：https://www.youtube.com/watch?v=ufHo8vbk6g4  \n",
    "了解 XGBoost 的目標函數是什麼，模型是怎麼樣進行優化  \n",
    "\n",
    "#### XGBoost 數學原理 slides - 英文\n",
    "連結：https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf  \n",
    "了解 XGBoost 的目標函數數學推導\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
